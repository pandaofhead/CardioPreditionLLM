#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Explanation Quality Evaluator using LLM API.
This script evaluates the quality of medical explanations generated by LLM
using another LLM instance as an evaluator.
"""

import os
import json
import time
from pathlib import Path
import openai
from dotenv import load_dotenv
from tqdm import tqdm
import logging
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import glob
import numpy as np
from openai import OpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Add the project root to the path
project_root = Path(__file__).resolve().parents[2]

# Evaluation criteria
EVALUATION_CRITERIA = {
    "medical_accuracy": {
        "description": "The explanation accurately reflects current medical knowledge and guidelines",
        "weight": 0.25
    },
    "clinical_relevance": {
        "description": "The explanation is relevant to the patient's specific condition and risk factors",
        "weight": 0.20
    },
    "actionability": {
        "description": "The explanation provides clear, actionable recommendations for healthcare providers",
        "weight": 0.20
    },
    "completeness": {
        "description": "The explanation covers all relevant aspects of the patient's condition",
        "weight": 0.15
    },
    "clarity": {
        "description": "The explanation is clear, well-structured, and easy to understand",
        "weight": 0.10
    },
    "guideline_alignment": {
        "description": "The explanation aligns with established medical guidelines (ACC/AHA)",
        "weight": 0.10
    }
}

def load_env():
    """Load environment variables from .env.local file"""
    env_path = os.path.join(project_root, '.env.local')
    load_dotenv(env_path)
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OpenAI API key not found in environment variables")
    return api_key

def get_llm_response(prompt, max_retries=3, delay=2):
    """Get response from OpenAI API with retry logic."""
    api_key = load_env()
    
    for attempt in range(max_retries):
        try:
            client = OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a medical expert evaluating explanations for cardiovascular disease risk predictions."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            if attempt == max_retries - 1:
                logging.error(f"Failed to get LLM response after {max_retries} attempts: {str(e)}")
                raise
            logging.warning(f"Attempt {attempt + 1} failed: {str(e)}")
            time.sleep(delay)

def generate_evaluation_prompt(explanation, patient_data):
    """Generate a prompt for the LLM to evaluate the explanation."""
    prompt = f"""Please evaluate the following medical explanation for a patient. 
Provide a score (0-10) and specific feedback for each criterion.

Patient Data:
{json.dumps(patient_data, indent=2)}

Explanation:
{explanation}

Please evaluate the explanation based on the following criteria:

1. Medical Accuracy (0-10):
Score: 
Feedback:

2. Clinical Relevance (0-10):
Score:
Feedback:

3. Actionability (0-10):
Score:
Feedback:

4. Completeness (0-10):
Score:
Feedback:

5. Clarity (0-10):
Score:
Feedback:

6. Guideline Alignment (0-10):
Score:
Feedback:

For each criterion:
- Score should be a number between 0 and 10
- Feedback should be specific and actionable
- Focus on the medical and clinical aspects of the explanation
"""
    return prompt

def parse_evaluation_response(response):
    """Parse the LLM evaluation response into scores and feedback."""
    scores = {}
    feedback = {}
    
    # Initialize default values
    criteria = [
        'medical_accuracy',
        'clinical_relevance',
        'actionability',
        'completeness',
        'clarity',
        'guideline_alignment'
    ]
    
    for criterion in criteria:
        scores[criterion] = 0.0
        feedback[criterion] = "No feedback provided"
    
    # Split response into sections
    sections = response.split('\n\n')
    
    for section in sections:
        section = section.strip()
        if not section:
            continue
            
        # Try to extract score and feedback
        score_match = re.search(r'Score:\s*(\d+(?:\.\d+)?)', section)
        feedback_match = re.search(r'Feedback:\s*(.+?)(?=\n|$)', section)
        
        # Determine which criterion this section is for
        current_criterion = None
        for criterion in criteria:
            if criterion.replace('_', ' ').lower() in section.lower():
                current_criterion = criterion
                break
        
        if current_criterion:
            if score_match:
                try:
                    score = float(score_match.group(1))
                    # Ensure score is between 0 and 10
                    score = max(0.0, min(10.0, score))
                    scores[current_criterion] = score
                except ValueError:
                    logging.warning(f"Could not parse score for {current_criterion}")
            
            if feedback_match:
                feedback[current_criterion] = feedback_match.group(1).strip()
    
    return scores, feedback

def evaluate_explanation(explanation, patient_data):
    """Evaluate a single explanation using the LLM."""
    # Generate evaluation prompt
    prompt = generate_evaluation_prompt(explanation, patient_data)
    
    # Get LLM response
    response = get_llm_response(prompt)
    
    # Parse response
    scores, feedback = parse_evaluation_response(response)
    
    return {
        'scores': scores,
        'feedback': feedback,
        'raw_response': response
    }

def generate_evaluation_visualizations(df, output_dir, explanation_type):
    """Generate visualizations for evaluation results"""
    # Set up the style
    plt.style.use('seaborn-v0_8-whitegrid')
    
    # Get criteria columns (excluding patient_id)
    criteria = [col for col in df.columns if col != 'patient_id']
    
    # Create a figure for criterion scores
    plt.figure(figsize=(12, 8))
    
    # Create a box plot for criterion scores
    data = [df[criterion] for criterion in criteria]
    
    plt.boxplot(data, labels=[criterion.replace('_', ' ').title() for criterion in criteria])
    plt.title(f'{explanation_type.title()} Explanation Evaluation - Criterion Scores')
    plt.ylabel('Score')
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Save the figure
    plt.savefig(os.path.join(output_dir, f"{explanation_type}_criterion_scores.png"))
    plt.close()
    
    # Create a figure for mean scores with error bars
    plt.figure(figsize=(12, 6))
    
    # Calculate mean and standard error for each criterion
    means = df[criteria].mean()
    sems = df[criteria].sem()
    
    # Create bar plot with error bars
    x = range(len(criteria))
    plt.bar(x, means, yerr=sems, capsize=5)
    plt.xticks(x, [criterion.replace('_', ' ').title() for criterion in criteria], rotation=45)
    plt.title(f'{explanation_type.title()} Explanation Evaluation - Mean Scores')
    plt.ylabel('Score')
    plt.ylim(0, 10)
    plt.tight_layout()
    
    # Save the figure
    plt.savefig(os.path.join(output_dir, f"{explanation_type}_mean_scores.png"))
    plt.close()
    
    # Create a heatmap of criterion scores
    plt.figure(figsize=(12, 8))
    
    # Create the heatmap
    sns.heatmap(df[criteria].T, annot=True, cmap='YlGnBu', fmt='.2f', 
                xticklabels=df['patient_id'],
                yticklabels=[criterion.replace('_', ' ').title() for criterion in criteria],
                vmin=0, vmax=10)
    plt.title(f'{explanation_type.title()} Explanation Evaluation - Criterion Heatmap')
    plt.xlabel('Patient ID')
    plt.ylabel('Criterion')
    plt.tight_layout()
    
    # Save the figure
    plt.savefig(os.path.join(output_dir, f"{explanation_type}_criterion_heatmap.png"))
    plt.close()
    
    # Create a radar plot
    plt.figure(figsize=(10, 10))
    
    # Calculate mean scores for radar plot
    mean_scores = df[criteria].mean()
    
    # Number of variables
    num_vars = len(criteria)
    
    # Compute angle for each axis
    angles = [n / float(num_vars) * 2 * np.pi for n in range(num_vars)]
    angles += angles[:1]
    
    # Initialize the spider plot
    ax = plt.subplot(111, polar=True)
    
    # Plot mean scores
    values = mean_scores.values.tolist()
    values += values[:1]
    ax.plot(angles, values)
    ax.fill(angles, values, alpha=0.25)
    
    # Set the labels
    plt.xticks(angles[:-1], [criterion.replace('_', ' ').title() for criterion in criteria])
    
    # Set chart title
    plt.title(f'{explanation_type.title()} Explanation Evaluation - Radar Plot')
    
    # Save the figure
    plt.savefig(os.path.join(output_dir, f"{explanation_type}_radar_plot.png"))
    plt.close()
    
    logging.info(f"Visualizations generated for {explanation_type} explanations")

def process_explanations(explanation_dir, output_dir, explanation_type):
    """Process all explanations in the directory."""
    # Get all explanation files
    explanation_files = glob.glob(os.path.join(explanation_dir, '*_counterfactual.json'))
    logging.info(f"Found {len(explanation_files)} {explanation_type} explanations to evaluate")
    
    all_results = {}
    
    # Set up progress bar
    with tqdm(total=len(explanation_files), desc=f"Evaluating {explanation_type} explanations") as pbar:
        for file_path in explanation_files:
            patient_id = os.path.basename(file_path).split('_')[1]  # Extract patient ID
            
            # Load explanation
            with open(file_path, 'r') as f:
                explanation_data = json.load(f)
            
            # Extract explanation and patient data from the JSON
            explanation = explanation_data['counterfactual_explanation']
            patient_data = {
                'patient_id': explanation_data['patient_id'],
                'key_modifiable_features': explanation_data['key_modifiable_features']
            }
            
            # Evaluate explanation
            logging.info(f"Evaluating explanation for patient {patient_id}")
            evaluation = evaluate_explanation(explanation, patient_data)
            
            # Save evaluation results
            result_file = os.path.join(output_dir, f"{patient_id}_evaluation.json")
            with open(result_file, 'w') as f:
                json.dump(evaluation, f, indent=2)
            logging.info(f"Evaluation saved for patient {patient_id}")
            
            # Store results
            all_results[patient_id] = {
                'scores': evaluation['scores'],
                'feedback': evaluation['feedback']
            }
            
            pbar.update(1)
    
    # Generate summary report
    generate_summary_report(all_results, output_dir, explanation_type)
    
    # Convert results to DataFrame for visualization
    data = []
    for patient_id, result in all_results.items():
        row = {'patient_id': patient_id}
        row.update(result['scores'])
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # Generate visualizations
    generate_evaluation_visualizations(df, output_dir, explanation_type)
    
    return all_results

def generate_summary_report(results, output_dir, explanation_type):
    """Generate a summary report of the evaluation results."""
    # Convert results to DataFrame
    data = []
    for patient_id, result in results.items():
        row = {'patient_id': patient_id}
        row.update(result['scores'])
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # Calculate statistics
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    stats = {
        'mean': df[numeric_columns].mean(),
        'std': df[numeric_columns].std(),
        'min': df[numeric_columns].min(),
        'max': df[numeric_columns].max(),
        'median': df[numeric_columns].median()
    }
    
    # Create summary report
    report = f"Evaluation Summary for {explanation_type} Explanations\n"
    report += "=" * 50 + "\n\n"
    
    for criterion in numeric_columns[1:]:  # Skip patient_id
        report += f"{criterion.replace('_', ' ').title()}:\n"
        report += f"  Mean: {stats['mean'][criterion]:.2f}\n"
        report += f"  Std: {stats['std'][criterion]:.2f}\n"
        report += f"  Min: {stats['min'][criterion]:.2f}\n"
        report += f"  Max: {stats['max'][criterion]:.2f}\n"
        report += f"  Median: {stats['median'][criterion]:.2f}\n\n"
    
    # Save report
    report_path = os.path.join(output_dir, 'summary_report.txt')
    with open(report_path, 'w') as f:
        f.write(report)
    
    logging.info(f"Summary report saved to {report_path}")
    
    # Save detailed results
    results_path = os.path.join(output_dir, 'detailed_results.csv')
    df.to_csv(results_path, index=False)
    logging.info(f"Detailed results saved to {results_path}")

def main():
    """Main function to run the evaluation process."""
    logging.info("Starting explanation evaluation...")
    
    # Set up directories
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    counterfactual_dir = os.path.join(project_root, 'models', 'llm', 'counterfactual_explanations')
    output_dir = os.path.join(project_root, 'models', 'llm', 'evaluation_results', 'counterfactual')
    os.makedirs(output_dir, exist_ok=True)
    
    # Process counterfactual explanations
    counterfactual_results = process_explanations(
        explanation_dir=counterfactual_dir,
        output_dir=output_dir,
        explanation_type='counterfactual'
    )

if __name__ == "__main__":
    main() 